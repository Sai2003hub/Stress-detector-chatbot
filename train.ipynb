{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset with stress levels saved to: updated_dataset_with_stress_levels.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def classify_stress_level(row):\n",
    "    # Stress indicators (higher values = more stress)\n",
    "    stress_score = (\n",
    "        row['lex_liwc_anx'] * 5.0 +       # Anxiety (high weight)\n",
    "        row['lex_liwc_anger'] * 4.5 +     # Anger\n",
    "        row['lex_liwc_sad'] * 4.0 +       # Sadness\n",
    "        row['lex_liwc_negemo'] * 3.5 +    # Negative emotion\n",
    "        row['lex_liwc_cogproc'] * 2.5 +   # Cognitive processing (rumination)\n",
    "        row['lex_liwc_health'] * 2.0 +    # Health concerns\n",
    "        row['lex_liwc_social'] * 1.8 +    # Social references\n",
    "        row['lex_dal_avg_pleasantness'] * (-2.0) # DAL: Lower pleasantness increases stress\n",
    "    )\n",
    "    \n",
    "    # Relief factors (higher values = less stress)\n",
    "    relief_score = (\n",
    "        row['lex_liwc_posemo'] * (-1.0) + # Reduced weight for positive emotion\n",
    "        row['lex_liwc_Authentic'] * (-0.8) # Reduced weight for authenticity\n",
    "    )\n",
    "    \n",
    "    # Final score (stress_score - relief_score)\n",
    "    score = stress_score + relief_score\n",
    "    \n",
    "    # Adjusted thresholds to balance distribution\n",
    "    if score > 30:\n",
    "        return 'Very High Stress'\n",
    "    elif score > 18:\n",
    "        return 'High Stress'\n",
    "    elif score > 8:\n",
    "        return 'Moderate Stress'\n",
    "    elif score > -2:\n",
    "        return 'Low Stress'\n",
    "    else:\n",
    "        return 'Very Low Stress'\n",
    "\n",
    "# Load data\n",
    "file_path = \"C:/Users/WinX/Downloads/train_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Apply classification\n",
    "df['stress_level'] = df.apply(classify_stress_level, axis=1)\n",
    "\n",
    "# Save results\n",
    "output_file_path = 'updated_dataset_with_stress_levels.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "print(f\"Updated dataset with stress levels saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting empath\n",
      "  Using cached empath-0.89.tar.gz (57 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from empath) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->empath) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->empath) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->empath) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->empath) (2025.1.31)\n",
      "Building wheels for collected packages: empath\n",
      "  Building wheel for empath (pyproject.toml): started\n",
      "  Building wheel for empath (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for empath: filename=empath-0.89-py3-none-any.whl size=57849 sha256=a7a83b7c652e8ab0e3b57736ccad4d0ed88746655f324bb26b73143f77b2b200\n",
      "  Stored in directory: c:\\users\\winx\\appdata\\local\\pip\\cache\\wheels\\92\\b3\\83\\9eb2c6199881e2385a59d99bd911363475060ebeb4bdb27242\n",
      "Successfully built empath\n",
      "Installing collected packages: empath\n",
      "Successfully installed empath-0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\WinX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WinX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "lexicon = Empath()\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_liwc_features(text):\n",
    "    analysis = lexicon.analyze(text, normalize=True)\n",
    "    sentiment = vader.polarity_scores(text)\n",
    "    \n",
    "    features = {\n",
    "        'lex_liwc_anx': analysis['nervousness'],\n",
    "        'lex_liwc_anger': analysis['anger'],\n",
    "        'lex_liwc_sad': analysis['sadness'],\n",
    "        'lex_liwc_negemo': sentiment['neg'],\n",
    "        'lex_liwc_posemo': sentiment['pos'],\n",
    "        'lex_liwc_cogproc': analysis['cognitive_mechanics'],\n",
    "        'lex_liwc_health': analysis['health'],\n",
    "        'lex_liwc_social': analysis['social'],\n",
    "        'lex_liwc_Authentic': analysis['honesty'],\n",
    "        'lex_dal_avg_pleasantness': sentiment['compound']\n",
    "    }\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_stress(text):\n",
    "    row = calculate_liwc_features(text)\n",
    "    stress_score = (\n",
    "        row['lex_liwc_anx'] * 5.0 +\n",
    "        row['lex_liwc_anger'] * 4.5 +\n",
    "        row['lex_liwc_sad'] * 4.0 +\n",
    "        row['lex_liwc_negemo'] * 3.5 +\n",
    "        row['lex_liwc_cogproc'] * 2.5 +\n",
    "        row['lex_liwc_health'] * 2.0 +\n",
    "        row['lex_liwc_social'] * 1.8 +\n",
    "        row['lex_dal_avg_pleasantness'] * (-2.0)\n",
    "    )\n",
    "    relief_score = (\n",
    "        row['lex_liwc_posemo'] * (-1.0) +\n",
    "        row['lex_liwc_Authentic'] * (-0.8)\n",
    "    )\n",
    "    score = stress_score + relief_score\n",
    "    \n",
    "    if score > 30:\n",
    "        return 'Very High Stress'\n",
    "    elif score > 18:\n",
    "        return 'High Stress'\n",
    "    elif score > 8:\n",
    "        return 'Moderate Stress'\n",
    "    elif score > -2:\n",
    "        return 'Low Stress'\n",
    "    else:\n",
    "        return 'Very Low Stress'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample sentence\n",
    "text = \"I am really anxious about my health and feeling overwhelmed by work.\"\n",
    "print(f\"Stress Level: {classify_stress(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress Level: Low Stress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\WinX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WinX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from empath import Empath\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "lexicon = Empath()\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_liwc_features(text):\n",
    "    analysis = lexicon.analyze(text, normalize=True)\n",
    "    sentiment = vader.polarity_scores(text)\n",
    "\n",
    "    features = {\n",
    "        'lex_liwc_anx': analysis.get('nervousness', 0.0),\n",
    "        'lex_liwc_anger': analysis.get('anger', 0.0),\n",
    "        'lex_liwc_sad': analysis.get('sadness', 0.0),\n",
    "        'lex_liwc_negemo': sentiment.get('neg', 0.0),\n",
    "        'lex_liwc_posemo': sentiment.get('pos', 0.0),\n",
    "        'lex_liwc_cogproc': analysis.get('thinking', 0.0),\n",
    "        'lex_liwc_health': analysis.get('health', 0.0),\n",
    "        'lex_liwc_social': analysis.get('social', 0.0),\n",
    "        'lex_liwc_Authentic': analysis.get('trust', 0.0),\n",
    "        'lex_dal_avg_pleasantness': sentiment.get('compound', 0.0)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def classify_stress(text):\n",
    "    row = calculate_liwc_features(text)\n",
    "    stress_score = (\n",
    "        row['lex_liwc_anx'] * 6.0 +\n",
    "        row['lex_liwc_anger'] * 5.5 +\n",
    "        row['lex_liwc_sad'] * 5.0 +\n",
    "        row['lex_liwc_negemo'] * 4.5 +\n",
    "        row['lex_liwc_cogproc'] * 3.5 +\n",
    "        row['lex_liwc_health'] * 3.0 +\n",
    "        row['lex_liwc_social'] * 2.5 +\n",
    "        row['lex_dal_avg_pleasantness'] * (-3.0)\n",
    "    )\n",
    "    relief_score = (\n",
    "        row['lex_liwc_posemo'] * (-1.5) +\n",
    "        row['lex_liwc_Authentic'] * (-1.2)\n",
    "    )\n",
    "    score = stress_score + relief_score\n",
    "\n",
    "    if score > 20:\n",
    "        return 'Very High Stress'\n",
    "    elif score > 12:\n",
    "        return 'High Stress'\n",
    "    elif score > 4:\n",
    "        return 'Moderate Stress'\n",
    "    elif score > -5:\n",
    "        return 'Low Stress'\n",
    "    else:\n",
    "        return 'Very Low Stress'\n",
    "\n",
    "text = \"I am really anxious about my health and feeling overwhelmed by work.\"\n",
    "print(f\"Stress Level: {classify_stress(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\winx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.1 MB 2.2 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.3/11.1 MB 2.2 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.8/11.1 MB 2.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.1 MB 2.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.7/11.1 MB 2.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.9/11.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.5/11.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.0/11.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.5/11.1 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.0/11.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.6/11.1 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.1/11.1 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.6/11.1 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.9/11.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.4/11.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.7/11.1 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.2/11.1 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.4/11.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.5/11.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.2-cp310-cp310-win_amd64.whl (41.2 MB)\n",
      "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/41.2 MB 2.4 MB/s eta 0:00:17\n",
      "    --------------------------------------- 0.8/41.2 MB 2.1 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 1.3/41.2 MB 1.9 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 1.6/41.2 MB 1.9 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 2.1/41.2 MB 2.0 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 2.4/41.2 MB 1.9 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 2.6/41.2 MB 1.8 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 3.1/41.2 MB 1.8 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 3.7/41.2 MB 1.9 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 3.9/41.2 MB 1.9 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 4.5/41.2 MB 1.9 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 4.7/41.2 MB 1.9 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 5.2/41.2 MB 1.9 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 5.5/41.2 MB 1.9 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 6.0/41.2 MB 1.9 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 6.3/41.2 MB 1.9 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 6.6/41.2 MB 1.9 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 7.1/41.2 MB 1.8 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 7.3/41.2 MB 1.8 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 7.9/41.2 MB 1.8 MB/s eta 0:00:19\n",
      "   -------- ------------------------------- 8.4/41.2 MB 1.9 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 8.7/41.2 MB 1.9 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 9.2/41.2 MB 1.9 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 9.4/41.2 MB 1.9 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 10.0/41.2 MB 1.9 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 10.5/41.2 MB 1.9 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 11.0/41.2 MB 1.9 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 11.3/41.2 MB 1.9 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 11.8/41.2 MB 1.9 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 12.3/41.2 MB 1.9 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 12.8/41.2 MB 1.9 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 13.4/41.2 MB 2.0 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 13.9/41.2 MB 2.0 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 14.2/41.2 MB 2.0 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 14.7/41.2 MB 2.0 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 15.2/41.2 MB 2.0 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 16.0/41.2 MB 2.0 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 16.5/41.2 MB 2.0 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 17.0/41.2 MB 2.1 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 17.6/41.2 MB 2.1 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 18.1/41.2 MB 2.1 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 18.6/41.2 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 19.1/41.2 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 19.7/41.2 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 20.2/41.2 MB 2.1 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 20.7/41.2 MB 2.1 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 21.2/41.2 MB 2.1 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 21.8/41.2 MB 2.2 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 22.3/41.2 MB 2.2 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 22.8/41.2 MB 2.2 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 23.6/41.2 MB 2.2 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 23.9/41.2 MB 2.2 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 24.4/41.2 MB 2.2 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 25.2/41.2 MB 2.2 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 25.7/41.2 MB 2.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 26.2/41.2 MB 2.2 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 26.7/41.2 MB 2.2 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 27.5/41.2 MB 2.2 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 27.8/41.2 MB 2.2 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 28.3/41.2 MB 2.2 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 28.8/41.2 MB 2.2 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 29.4/41.2 MB 2.2 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 29.9/41.2 MB 2.2 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 30.4/41.2 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 30.9/41.2 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 31.2/41.2 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 31.7/41.2 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 32.2/41.2 MB 2.3 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 32.8/41.2 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 33.0/41.2 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 33.6/41.2 MB 2.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 34.1/41.2 MB 2.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 34.6/41.2 MB 2.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 35.1/41.2 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 35.7/41.2 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 36.2/41.2 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 36.7/41.2 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 37.2/41.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 37.7/41.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.5/41.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.1/41.2 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.6/41.2 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 40.1/41.2 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.6/41.2 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.2 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.2/41.2 MB 2.3 MB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"d1.csv\")\n",
    "\n",
    "# Extract text and labels\n",
    "texts = df['text'].tolist()\n",
    "labels = df['stress_level'].map({\n",
    "    \"Very Low Stress\": 0,\n",
    "    \"Low Stress\": 1,\n",
    "    \"Moderate Stress\": 2,\n",
    "    \"High Stress\": 3,\n",
    "    \"Very High Stress\": 4\n",
    "}).tolist()\n",
    "\n",
    "# Split data (stratified)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WinX\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use DistilBERT (smaller than BERT/ROBERTA)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize with shorter max_length (reduces GPU memory)\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StressDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = StressDataset(train_encodings, train_labels)\n",
    "test_dataset = StressDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load DistilBERT model with 5 classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=5\n",
    ")\n",
    "\n",
    "# Training arguments (optimized for low memory)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",  # Matches compute_metrics\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,  # Add this line\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:02<00:00, 19.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 1.0518702268600464, 'eval_accuracy': 0.6189427312775331, 'eval_runtime': 3.3101, 'eval_samples_per_second': 137.158, 'eval_steps_per_second': 17.22, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test dataset\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "logits, labels = test_predictions.predictions, test_predictions.label_ids\n",
    "predicted_labels = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(\n",
    "    labels, predicted_labels, \n",
    "    target_names=[\n",
    "        \"Very Low Stress\", \"Low Stress\", \"Moderate Stress\",\n",
    "        \"High Stress\", \"Very High Stress\"\n",
    "    ]\n",
    ")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_bert_model\\\\tokenizer_config.json',\n",
       " './saved_bert_model\\\\special_tokens_map.json',\n",
       " './saved_bert_model\\\\vocab.txt',\n",
       " './saved_bert_model\\\\added_tokens.json',\n",
       " './saved_bert_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./saved_bert_model\")\n",
    "tokenizer.save_pretrained(\"./saved_bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very Low Stress\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load saved model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./saved_bert_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./saved_bert_model\")\n",
    "\n",
    "def predict_stress(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    # Predict\n",
    "    outputs = model(**inputs)\n",
    "    predicted_label = outputs.logits.argmax(-1).item()\n",
    "    # Map label index to stress level\n",
    "    label_map = {\n",
    "        0: \"Very Low Stress\",\n",
    "        1: \"Low Stress\",\n",
    "        2: \"Moderate Stress\",\n",
    "        3: \"High Stress\",\n",
    "        4: \"Very High Stress\"\n",
    "    }\n",
    "    return label_map[predicted_label]\n",
    "\n",
    "# Example usage\n",
    "new_text = \"The sun is shining, the breeze is gentle, and everything is moving at its own pace.\"\n",
    "print(predict_stress(new_text))  # Output: \"High Stress\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
